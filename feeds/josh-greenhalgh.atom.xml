<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>My Blog</title><link href="http://www.southampton.ac.uk/~jdg1g14/" rel="alternate"></link><link href="http://www.southampton.ac.uk/~jdg1g14/feeds/josh-greenhalgh.atom.xml" rel="self"></link><id>http://www.southampton.ac.uk/~jdg1g14/</id><updated>2015-02-13T00:00:00+01:00</updated><entry><title>An Introduction to the wonderful world of Support Vector Machines...</title><link href="http://www.southampton.ac.uk/~jdg1g14/SVM.html" rel="alternate"></link><updated>2015-02-13T00:00:00+01:00</updated><author><name>Josh Greenhalgh</name></author><id>tag:www.southampton.ac.uk,2015-02-13:~jdg1g14/SVM.html</id><summary type="html">&lt;div class="section" id="what-is-the-problem-we-would-like-svm-s-to-solve"&gt;
&lt;h2&gt;What is the problem we would like SVM's to solve?&lt;/h2&gt;
&lt;p&gt;So you have some friends, they comes in two types - people who you trust to look after your cat and people who under no circumstances should be left alone with your cat. You acquire a new friend but how are you going to tell if they should be trusted with your cat?! You decide to give your current friends some ratings on various personal qualities;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;How likely are they to wear odd socks? (a probability),&lt;/li&gt;
&lt;li&gt;To what extent do they appreciate the comedic genius of Borat? (on a scale of 10).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These qualities where chosen since as everyone knows odd sock wearing fans of Borat make for very happy cats.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/cat_happiness.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Now the question we want to answer is how do we decide if we trust our new friend? We want a decision rule that is sensitive to the actual values of our friends personal qualities, which we are confident will not make our cat sad by introducing it to an unsuitable character!&lt;/p&gt;
&lt;p&gt;The above discussion is my attempt at being light-hearted, probably a failure but hey ho! The basic elements of the problem we would like to solve with SVM's are there but we need to put them on a slightly more mathematical foundation...&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-little-bit-more-mathematical"&gt;
&lt;h2&gt;A little bit more mathematical...&lt;/h2&gt;
&lt;p&gt;In all that follows we will work in &lt;span class="math"&gt;\(\mathbb{R}^2\)&lt;/span&gt; but the arguments transfer to higher dimensions.&lt;/p&gt;
&lt;p&gt;The first thing we will do is form a vector &lt;span class="math"&gt;\(\mathbf{x}_i\)&lt;/span&gt; for each sample that will contain the values which we have decided to use for the purpose of discrimination. We need to define two sets; one for positive samples &lt;span class="math"&gt;\(X^+\)&lt;/span&gt;  and one for negative samples &lt;span class="math"&gt;\(X^-\)&lt;/span&gt;. Our goal is to decide which set an unknown sample &lt;span class="math"&gt;\(\mathbf{u}\)&lt;/span&gt; belongs to.&lt;/p&gt;
&lt;p&gt;As is often the case with mathematical arguments the best strategy is to assume you already have the thing you want - the hope is that this assumption will give you what you want! Clearly this is a very circular argument however this is how we will proceed...&lt;/p&gt;
&lt;p&gt;We are going to assume that their exists some median line which separates the positive samples from the negative samples, we will think of this as the centre of a road such that the positive samples are on the right hand side and the negative samples are on the left.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/median_and_margin.png" /&gt;
&lt;/div&gt;
&lt;p&gt;The width of this road should be as large as possible - this is what gives us confidence in our decision rule since we are aiming for the largest separation of the two sets.&lt;/p&gt;
&lt;p&gt;Now we will introduce a vector &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; that will be constrained to be perpendicular to the median line of the road. The length of &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; is as yet unknown, but as we will see shortly will play an important role. For the time being this vector is going to act as a ruler that will allow us to measure how far on a particular side of the road some unknown sample &lt;span class="math"&gt;\(\mathbf{u}\)&lt;/span&gt; lies.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/decision_unknown.png" /&gt;
&lt;/div&gt;
&lt;p&gt;The larger the projection of &lt;span class="math"&gt;\(\mathbf{u}\)&lt;/span&gt; onto &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; is then the further to the right of the median line the unknown sample lies. By setting some threshold &lt;span class="math"&gt;\(c\)&lt;/span&gt; we can form the following decision rules;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\mathbf{u}\cdot\mathbf{w} &amp;amp;\ge c \implies \mathbf{u} \in X^+ \\
\mathbf{u}\cdot\mathbf{w} &amp;amp;\le c \implies \mathbf{u} \in X^-
\end{align*}
&lt;/div&gt;
&lt;p&gt;Which is equivalent to the following where &lt;span class="math"&gt;\(b = -c\)&lt;/span&gt;;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\mathbf{u}\cdot\mathbf{w} +b &amp;amp;\ge 0 \implies \mathbf{u} \in X^+ \\
\mathbf{u}\cdot\mathbf{w} +b &amp;amp;\le 0 \implies \mathbf{u} \in X^-
\end{align*}
&lt;/div&gt;
&lt;p&gt;So at this point we would like to find a particular &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; such that the width of the road is as large as possible. Unfortunately we are not yet in a position to do this - we need to impose some further constraints. We have already constrained &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; to be perpendicular to the median of the road but this is not quite enough. In order to impose some more constraints we are going to say that the following must be true for known samples (the friends we already know whether to trust with our cat);&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\mathbf{x}^+\cdot\mathbf{w} +b &amp;amp;\ge 1  \\
\mathbf{x}^-\cdot\mathbf{w} +b &amp;amp;\le -1
\end{align*}
&lt;/div&gt;
&lt;p&gt;Now it would be great if we could have a single constraint rather than the two we have just created, well with the judicial introduction of yet another variable we can do just that. We will define the following;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
   y_i =  \left \{
\begin{array}{lr}
  1 &amp;amp; : x_i \in X^+\\
  -1 &amp;amp; : x_i \in X^-
\end{array}
\right.
\end{equation*}
&lt;/div&gt;
&lt;p&gt;With a little bit of thought it should become crystal clear (it took me a while...) that we can remove the dependence on the particular type of sample from the two constraints by multiplying by this new variable - leaving us with the following constraint that should be satisfied for all known samples;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
y_i(\mathbf{x}_i\cdot\mathbf{w} +b) - 1 \ge 0
\end{equation*}
&lt;/div&gt;
&lt;p&gt;We are going to go one step further and say that for any samples that lie on the edge of the road the above constraint is going to become an equality constraint;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
y_i(\mathbf{x}_i\cdot\mathbf{w} +b) - 1 = 0
\end{equation*}
&lt;/div&gt;
&lt;p&gt;So we have a set of constraints and we have loosely talked about some quantity (the width of the road) that we would like to maximise. In order to continue we need to find some expression for the width of the road.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/road_width.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Lets look at two known samples, one positive &lt;span class="math"&gt;\(\mathbf{x}^+\)&lt;/span&gt; and the other negative &lt;span class="math"&gt;\(\mathbf{x}^-\)&lt;/span&gt;, that lie smack bang on the edge of the road. If we had some kind of ruler we could use the difference between the two samples &lt;span class="math"&gt;\(\mathbf{x}^+ - \mathbf{x}^-\)&lt;/span&gt; to calculate the width of the road. Well we have just such a ruler the vector &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt;. All we need to do is normalise this vector and project the difference between the two samples onto it to find the width of the road;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
width &amp;amp;= (\mathbf{x}^+ - \mathbf{x}^-)\cdot \frac{\mathbf{w}}{\|\mathbf{w}\|} \\
          &amp;amp;= \mathbf{x}^+ \cdot \frac{\mathbf{w}}{\|\mathbf{w}\|} - \mathbf{x}^- \cdot \frac{\mathbf{w}}{\|\mathbf{w}\|}
\end{align*}
&lt;/div&gt;
&lt;p&gt;Now with a little bit of help from our constraints (for sample on the edge of the road) we see that;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\mathbf{x}^+\cdot\mathbf{w} +b - 1 = 0 &amp;amp;\implies \mathbf{x}^+\cdot\mathbf{w} =1-b \\
\mathbf{x}^-\cdot\mathbf{w} -b - 1 = 0 &amp;amp;\implies \mathbf{x}^+\cdot\mathbf{w} =1+b
\end{align*}
&lt;/div&gt;
&lt;p&gt;Which gives us;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
width &amp;amp;= \frac{1-b}{\|\mathbf{w}\|} + \frac{1+b}{\|\mathbf{w}\|} \\
      &amp;amp;= \frac{2}{\|\mathbf{w}\|}
\end{align*}
&lt;/div&gt;
&lt;p&gt;A remarkable result - the width of our road is completely determined by the length of &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt;. So we nearly have everything we need to solve our problem.&lt;/p&gt;
&lt;p&gt;However we need to perform some more mathematical trickery, our objective function (the quantity we want to maximise) is not a nice easy optimisation problem.&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\max_{\mathbf{w}} \frac{2}{\|\mathbf{w}\|} &amp;amp;= \max_{\mathbf{w}} \frac{1}{\|\mathbf{w}\|} \\
                                           &amp;amp;= \min_{\mathbf{w}} \|\mathbf{w}\| \\
                                           &amp;amp;= \min_{\mathbf{w}} \frac{1}{2}\|\mathbf{w}\|^2
\end{align*}
&lt;/div&gt;
&lt;p&gt;This new minimisation problem has some very nice properties which make SVM's such a powerful technique. It is known as a convex optimisation problem and has the beautiful property that it has a single global minimum!&lt;/p&gt;
&lt;p&gt;So we want to find;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\min_{\mathbf{w}} \frac{1}{2}\|\mathbf{w}\|^2 \quad \text{subject to} \quad  y_i(\mathbf{x}_i\cdot\mathbf{w} +b) - 1 = 0
\end{equation*}
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="optimisation"&gt;
&lt;h2&gt;Optimisation&lt;/h2&gt;
&lt;p&gt;Whenever we have some function we wish to minimise along with a set of constraints we call upon one of the greats - Lagrange. The Lagrangian is a new function that we will create from our objective and constraints for which we shall find a minimum. Our Lagrangian function will be;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
L = \frac{1}{2}\|\mathbf{w}\|^2 - \sum_i a_i(y_i(\mathbf{x}_i\cdot\mathbf{w} +b) - 1)
\end{equation*}
&lt;/div&gt;
&lt;p&gt;To find a minimum of this function we need to find where its derivatives (with respect to anything that varies - in our case &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt;) vanish. Derivatives with respect to vectors can sometimes seem a little daunting but the best course of action is to write the Lagrangian out in component form and then calculate the derivatives - once you do this you will see that;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\frac{\partial L}{\partial \mathbf{w}} &amp;amp;= \mathbf{w} - \sum_i a_i y_i x_i\\
\frac{\partial L}{\partial b} &amp;amp;= -\sum_i a_i y_i
\end{align*}
&lt;/div&gt;
&lt;p&gt;If we set these derivatives equal to zero we obtain the following relations;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\mathbf{w} &amp;amp;= \sum_i a_i y_i x_i\\
0 &amp;amp;= \sum_i a_i y_i
\end{align*}
&lt;/div&gt;
&lt;p&gt;The first of these two relations tells us something fairly interesting about our problem - it tells us that the vector &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; we are looking for is a linear combination of the samples we already know. The second relation will be of particular use as we dig further into this problem and find the most useful property of the SVM method.&lt;/p&gt;
&lt;p&gt;Now that we have an expression for &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; in terms of our samples we are going to take another look at the Lagrangian, except this time we are going to remove (using our new relation) all reference to &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt;. By substitution we see a new form of the Lagrangian;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
L = \frac{1}{2}\sum_i a_i y_i x_i \cdot \sum_j a_j y_j x_j - \sum_i a_i y_i x_i \cdot \sum_j a_j y_j x_j - b\sum_i a_i y_i + \sum_i a_i
\end{equation*}
&lt;/div&gt;
&lt;p&gt;The second relation we obtained from our derivatives shows us that the third term in this expression is zero, and we notice that the first two terms are multiples of the same thing. This leaves us with the following;&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
L = \sum_i a_i - \frac{1}{2}\sum_i \sum_j a_i a_j y_i y_j x_i \cdot x_j
\end{equation*}
&lt;/div&gt;
&lt;p&gt;This Lagrangian highlights the most significant feature of SVM's - the optimization problem is completely determined by the dot product of pairs of known samples. This allows for solving decision problems that are not linearly separable in the space spanned by the quantities we choose to use for discrimination. All we need to know is how to calculate dot products in some transformed space in which the samples are in fact linearly separable. This is done by constructing what are known as kernel functions that give us the value of the dot product in some higher dimensional space using only the known samples without the need to even know how they have been transformed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="final-thoughts"&gt;
&lt;h2&gt;Final thoughts...&lt;/h2&gt;
&lt;p&gt;BLAH&lt;/p&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }, linebreaks: { automatic: false, width: 'container' }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Machine-Learning"></category></entry></feed>